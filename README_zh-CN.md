## ä¹¦ç”Ÿ Intern-S1

<div align="center">
<img src="./assets/title_zh.jpg" />

<div>&nbsp;</div>

[ğŸ¤—Huggingface](https://huggingface.co/collections/internlm/intern-s1-6882e325e8ac1c58ba108aa5) â€¢  [<img src="./assets/modelscope_logo.png" width="20px" /> ModelScope](https://modelscope.cn/collections/Intern-S1-29b3100f15e240) â€¢ [ğŸ“œæŠ€æœ¯æŠ¥å‘Š (å³å°†å…¬å¼€)](<>) â€¢ [ğŸ’¬åœ¨çº¿ä½“éªŒ](https://chat.intern-ai.org.cn/)

[English](./README.md) |
[ç®€ä½“ä¸­æ–‡](./README_zh-CN.md)

</div>

<p align="center">
    ğŸ‘‹ join us on <a href="https://discord.gg/xa29JuW87d" target="_blank">Discord</a> and <a href="https://cdn.vansin.top/intern-s1.jpg" target="_blank">WeChat</a>
</p>

## ç®€ä»‹

æˆ‘ä»¬æ¨å‡ºäº† **Intern-S1**ï¼Œè¿™æ˜¯æˆ‘ä»¬æ¨å‡ºçš„**æœ€å…ˆè¿›çš„å¼€æºå¤šæ¨¡æ€æ¨ç†æ¨¡å‹**ã€‚Intern-S1 åœ¨å…·å¤‡å¼ºå¤§é€šç”¨ä»»åŠ¡èƒ½åŠ›çš„åŒæ—¶ï¼Œåœ¨å¹¿æ³›çš„**ç§‘å­¦ä»»åŠ¡ä¸­ä¹Ÿè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½**ï¼Œå¯ä¸æœ€å…ˆè¿›çš„é—­æºå•†ä¸šæ¨¡å‹ç›¸åª²ç¾ã€‚

Intern-S1 åŸºäºä¸€ä¸ª 235B çš„ MoE è¯­è¨€æ¨¡å‹ (Qwen3) å’Œä¸€ä¸ª 6B çš„è§†è§‰ç¼–ç å™¨ (InternViT) æ„å»ºï¼Œå¹¶åœ¨ **5T token** çš„å¤šæ¨¡æ€æ•°æ®ä¸Šè¿›è¡Œäº†ç»­è®­ï¼Œå…¶ä¸­åŒ…å«**è¶…è¿‡ 2.5T çš„ç§‘å­¦é¢†åŸŸ token**ã€‚è¿™ä¸€è®­ç»ƒç­–ç•¥ä½¿å¾—è¯¥æ¨¡å‹ä¸ä»…ä¿ç•™äº†å¼ºå¤§çš„é€šç”¨èƒ½åŠ›ï¼Œè¿˜åœ¨ä¸“ä¸šç§‘å­¦ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¾‹å¦‚**è§£æåŒ–å­¦ç»“æ„ã€ç†è§£è›‹ç™½è´¨åºåˆ—ã€è§„åˆ’åŒ–åˆç‰©åˆæˆè·¯å¾„**ï¼Œä½¿ Intern-S1 æˆä¸ºäº†èƒ½å¤Ÿåº”å¯¹çœŸå®ç§‘ç ”ä»»åŠ¡çš„ AI åŠ©æ‰‹ã€‚

å¦å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº† **Intern-S1-mini**ï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨äº† Intern-S1 åŒæ ·è®­ç»ƒæŠ€æœ¯çš„è½»é‡çº§æ¨¡å‹ï¼ŒåŒ…å«ä¸€ä¸ª 8B çš„è¯­è¨€æ¨¡å‹å’Œä¸€ä¸ª 400M çš„è§†è§‰ç¼–ç å™¨ã€‚

### ç‰¹æ€§

- åœ¨è¯­è¨€ä¸è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å¼ºåŠ²ï¼Œå°¤å…¶æ“…é•¿ç§‘å­¦ä»»åŠ¡ã€‚

- åœ¨åŒ…å«è¶…è¿‡ 50% ç§‘å­¦ä¸“ä¸šæ•°æ®çš„ 5T è§„æ¨¡æ•°æ®é›†ä¸ŠæŒç»­é¢„è®­ç»ƒï¼Œæ·±åº¦èåˆä¸“ä¸šé¢†åŸŸçŸ¥è¯†ã€‚

- åŠ¨æ€åˆ†è¯å™¨åŸç”Ÿæ”¯æŒå¯¹åˆ†å­å¼ã€è›‹ç™½è´¨åºåˆ—ã€åœ°éœ‡ä¿¡å·ç­‰æ•°æ®çš„ç†è§£ã€‚

## æ¨¡å‹åº“

### Intern-S1

|                                                                    | BF16                                                                                              | FP8                                                                                                       | GGUF                                                                                                        |
| ------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- |
| ğŸ¤—HuggingFace                                                      | [internlm/Intern-S1](https://huggingface.co/internlm/Intern-S1)                                   | [internlm/Intern-S1-FP8](https://huggingface.co/internlm/Intern-S1-FP8)                                   | [internlm/Intern-S1-GGUF](https://huggingface.co/internlm/Intern-S1-GGUF)                                   |
| <img src="./assets/modelscope_logo.png" width="20px" /> ModelScope | [Shanghai_AI_Laboratory/Intern-S1](https://modelscope.cn/models/Shanghai_AI_Laboratory/Intern-S1) | [Shanghai_AI_Laboratory/Intern-S1-FP8](https://modelscope.cn/models/Shanghai_AI_Laboratory/Intern-S1-FP8) | [Shanghai_AI_Laboratory/Intern-S1-GGUF](https://modelscope.cn/models/Shanghai_AI_Laboratory/Intern-S1-GGUF) |

### Intern-S1-mini

|                                                                    | BF16                                                                                                        | FP8                                                                                                                 | GGUF                                                                                |
| ------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------- |
| ğŸ¤—HuggingFace                                                      | [internlm/Intern-S1-mini](https://huggingface.co/internlm/Intern-S1-mini)                                   | [internlm/Intern-S1-mini-FP8](https://huggingface.co/internlm/Intern-S1-mini-FP8)                                   | [internlm/Intern-S1-mini-GGUF](https://huggingface.co/internlm/Intern-S1-mini-GGUF) |
| <img src="./assets/modelscope_logo.png" width="20px" /> ModelScope | [Shanghai_AI_Laboratory/Intern-S1-mini](https://modelscope.cn/models/Shanghai_AI_Laboratory/Intern-S1-mini) | [Shanghai_AI_Laboratory/Intern-S1-mini-FP8](https://modelscope.cn/models/Shanghai_AI_Laboratory/Intern-S1-mini-FP8) | -                                                                                   |

## æ€§èƒ½è¯„ä¼°

æˆ‘ä»¬åœ¨å¤šä¸ªé€šç”¨æ•°æ®é›†å’Œç§‘å­¦æ•°æ®é›†ä¸Šè¯„ä¼°äº† Intern-S1 çš„è¡¨ç°ï¼Œå¹¶ä¸è¿‘æœŸçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œäº†å¯¹æ¯”ï¼Œç»“æœå¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚

### Intern-S1

<table>
  <thead>
    <tr>
      <th rowspan="2">è¯„æµ‹é›†</th>
      <th colspan="2">Intern-S1</th>
      <th>InternVL3-78B</th>
      <th>Qwen2.5-VL-72B</th>
      <th>DS-R1-0528</th>
      <th>Qwen3-235B-A22B</th>
      <th>Kimi-K2-Instruct</th>
      <th>Gemini-2.5 Pro</th>
      <th>o3</th>
      <th>Grok-4</th>
    </tr>
  </thead>
  <tbody>
    <tr><td>MMLU-Pro</td><td colspan="2">83.5 âœ…</td><td>73.0</td><td>72.1</td><td>83.4</td><td>82.2</td><td>82.7</td><td>86.0</td><td>85.0</td><td>85.9</td></tr>
    <tr><td>MMMU</td><td colspan="2">77.7 âœ…</td><td>72.2</td><td>70.2</td><td>-</td><td>-</td><td>-</td><td>81.9</td><td>80.8</td><td>77.9</td></tr>
    <tr><td>GPQA</td><td colspan="2">77.3</td><td>49.9</td><td>49.0</td><td>80.6</td><td>71.1</td><td>77.8</td><td>83.8</td><td>83.3</td><td>87.5</td></tr>
    <tr><td>MMStar</td><td colspan="2">74.9 âœ…</td><td>72.5</td><td>70.8</td><td>-</td><td>-</td><td>-</td><td>79.3</td><td>75.1</td><td>69.6</td></tr>
    <tr><td>MathVista</td><td colspan="2">81.5 ğŸ‘‘</td><td>79.0</td><td>74.8</td><td>-</td><td>-</td><td>-</td><td>80.3</td><td>77.5</td><td>72.5</td></tr>
    <tr><td>AIME2025</td><td colspan="2">86.0</td><td>10.7</td><td>10.9</td><td>87.5</td><td>81.5</td><td>51.4</td><td>83.0</td><td>88.9</td><td>91.7</td></tr>
    <tr><td>MathVision</td><td colspan="2">62.5 âœ…</td><td>43.1</td><td>38.1</td><td>-</td><td>-</td><td>-</td><td>73.0</td><td>67.7</td><td>67.3</td></tr>
    <tr><td>IFEval</td><td colspan="2">86.7</td><td>75.6</td><td>83.9</td><td>79.7</td><td>85.0</td><td>90.2</td><td>91.5</td><td>92.2</td><td>92.8</td></tr>
    <tr><td>SFE</td><td colspan="2">44.3 ğŸ‘‘</td><td>36.2</td><td>30.5</td><td>-</td><td>-</td><td>-</td><td>43.0</td><td>37.7</td><td>31.2</td></tr>
    <tr><td>Physics</td><td colspan="2">44.0 âœ…</td><td>23.1</td><td>15.7</td><td>-</td><td>-</td><td>-</td><td>40.0</td><td>47.9</td><td>42.8</td></tr>
    <tr><td>SmolInstruct</td><td colspan="2">51.0 ğŸ‘‘</td><td>19.4</td><td>21.0</td><td>30.7</td><td>28.7</td><td>48.1</td><td>40.4</td><td>43.9</td><td>47.3</td></tr>
    <tr><td>ChemBench</td><td colspan="2">83.4 ğŸ‘‘</td><td>61.3</td><td>61.6</td><td>75.6</td><td>75.8</td><td>75.3</td><td>82.8</td><td>81.6</td><td>83.3</td></tr>
    <tr><td>MatBench</td><td colspan="2">75.0 ğŸ‘‘</td><td>49.3</td><td>51.5</td><td>57.7</td><td>52.1</td><td>61.7</td><td>61.7</td><td>61.6</td><td>67.9</td></tr>
    <tr><td>MicroVQA</td><td colspan="2">63.9 ğŸ‘‘</td><td>59.1</td><td>53.0</td><td>-</td><td>-</td><td>-</td><td>63.1</td><td>58.3</td><td>59.5</td></tr>
    <tr><td>ProteinLMBench</td><td colspan="2">63.1</td><td>61.6</td><td>61.0</td><td>61.4</td><td>59.8</td><td>66.7</td><td>62.9</td><td>67.7</td><td>66.2</td></tr>
    <tr><td>MSEarthMCQ</td><td colspan="2">65.7 ğŸ‘‘</td><td>57.2</td><td>37.6</td><td>-</td><td>-</td><td>-</td><td>59.9</td><td>61.0</td><td>58.0</td></tr>
    <tr><td>XLRS-Bench</td><td colspan="2">55.0 ğŸ‘‘</td><td>49.3</td><td>50.9</td><td>-</td><td>-</td><td>-</td><td>45.2</td><td>43.6</td><td>45.4</td></tr>
  </tbody>
</table>

> **æ³¨æ„**: âœ… è¡¨ç¤ºåœ¨å¼€æºæ¨¡å‹ä¸­å–å¾—æœ€ä¼˜ï¼Œ ğŸ‘‘ è¡¨ç¤ºåœ¨æ‰€æœ‰æ¨¡å‹ä¸­å–å¾—æœ€ä¼˜ã€‚

### Intern-S1-mini

| è¯„æµ‹é›†         | Intern-S1-mini | Qwen3-8B | GLM-4.1V | MiMo-VL-7B-RL-2508 |
| -------------- | -------------- | -------- | -------- | ------------------ |
| MMLU-Pro       | **74.78**      | 73.7     | 57.1     | 73.93              |
| MMMU           | **72.33**      | N/A      | 69.9     | 70.4               |
| MMStar         | 65.2           | N/A      | 71.5     | 72.9               |
| GPQA           | **65.15**      | 62       | 50.32    | 60.35              |
| AIME2024       | **84.58**      | 76       | 36.2     | 72.6               |
| AIME2025       | **80**         | 67.3     | 32       | 64.4               |
| MathVision     | 51.41          | N/A      | 53.9     | 54.5               |
| MathVista      | 70.3           | N/A      | 80.7     | 79.4               |
| IFEval         | 81.15          | 85       | 71.53    | 71.4               |
| SFE            | 35.84          | N/A      | 43.2     | 43.9               |
| Physics        | **28.76**      | N/A      | 4.3      | 23.9               |
| SmolInstruct   | **32.2**       | 17.6     | 18.1     | 16.11              |
| ChemBench      | **76.47**      | 61.1     | 56.2     | 66.78              |
| MatBench       | **61.55**      | 45.24    | 54.3     | 46.9               |
| MicroVQA       | **56.62**      | N/A      | 50.2     | 50.96              |
| ProteinLMBench | 58.47          | 59.1     | 58.3     | 59.8               |
| MSEarthMCQ     | **58.12**      | N/A      | 50.3     | 47.3               |
| XLRS-Bench     | **51.63**      | N/A      | 49.8     | 12.29              |

è¯„ä¼°ä½¿ç”¨äº† [OpenCompass](https://github.com/open-compass/OpenCompass/) å’Œ [VLMEvalkit](https://github.com/open-compass/vlmevalkit)ã€‚

## å¿«é€Ÿå¼€å§‹

### é‡‡æ ·å‚æ•°å»ºè®®

æˆ‘ä»¬æ¨èä½¿ç”¨å¦‚ä¸‹çš„è¶…å‚æ•°ä»¥è·å¾—æ›´å¥½çš„ç”Ÿæˆæ•ˆæœï¼š

Intern-S1:

```python
top_p = 1.0
top_k = 50
min_p = 0.0
temperature = 0.7
```

Intern-S1-mini:

```python
top_p = 1.0
top_k = 50
min_p = 0.0
temperature = 0.8
```

### Transformers ç¤ºä¾‹

ä¸‹é¢æ˜¯ä½¿ç”¨æ–‡æœ¬å’Œå¤šæ¨¡æ€è¾“å…¥è¿›è¡Œæ¨ç†ç”Ÿæˆçš„ç¤ºä¾‹ä»£ç ã€‚

> **è¯·ä½¿ç”¨ transformers >= 4.53.0 çš„ç‰ˆæœ¬ä»¥ç¡®ä¿æ¨¡å‹æ­£å¸¸è¿è¡Œã€‚**

#### æ–‡æœ¬è¾“å…¥

```python
from transformers import AutoProcessor, AutoModelForCausalLM
import torch

model_name = "internlm/Intern-S1"
processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", torch_dtype="auto", trust_remote_code=True)

messages = [
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "tell me about an interesting physical phenomenon."},
        ],
    }
]

inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors="pt").to(model.device, dtype=torch.bfloat16)

generate_ids = model.generate(**inputs, max_new_tokens=32768)
decoded_output = processor.decode(generate_ids[0, inputs["input_ids"].shape[1] :], skip_special_tokens=True)
print(decoded_output)
```

#### å›¾åƒè¾“å…¥

```python
from transformers import AutoProcessor, AutoModelForCausalLM
import torch

model_name = "internlm/Intern-S1"
processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", torch_dtype="auto", trust_remote_code=True)

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "url": "http://images.cocodataset.org/val2017/000000039769.jpg"},
            {"type": "text", "text": "Please describe the image explicitly."},
        ],
    }
]

inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors="pt").to(model.device, dtype=torch.bfloat16)

generate_ids = model.generate(**inputs, max_new_tokens=32768)
decoded_output = processor.decode(generate_ids[0, inputs["input_ids"].shape[1] :], skip_special_tokens=True)
print(decoded_output)
```

#### è§†é¢‘è¾“å…¥

è¯·ç¡®ä¿å®‰è£…äº† decord è§†é¢‘è§£ç åº“ï¼Œå¯é€šè¿‡ `pip install decord` å®‰è£…ã€‚

```python
from transformers import AutoProcessor, AutoModelForCausalLM
import torch

model_name = "internlm/Intern-S1"
processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", torch_dtype="auto", trust_remote_code=True)

messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "video",
                    "url": "https://huggingface.co/datasets/hf-internal-testing/fixtures_videos/resolve/main/tennis.mp4",
                },
                {"type": "text", "text": "What type of shot is the man performing?"},
            ],
        }
    ]

inputs = processor.apply_chat_template(
        messages,
        return_tensors="pt",
        add_generation_prompt=True,
        video_load_backend="decord",
        tokenize=True,
        return_dict=True,
    ).to(model.device, dtype=torch.float16)

generate_ids = model.generate(**inputs, max_new_tokens=32768)
decoded_output = processor.decode(generate_ids[0, inputs["input_ids"].shape[1] :], skip_special_tokens=True)
print(decoded_output)
```

### éƒ¨ç½²æœåŠ¡

åœ¨éƒ¨ç½² InternS1 ç³»åˆ—æ¨¡å‹æ—¶ï¼Œå¯¹äºç¡¬ä»¶çš„æœ€ä½è¦æ±‚å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š

|                                       Model                                       | A100(GPUs) | H800(GPUs) | H100(GPUs) | H200(GPUs) |
| :-------------------------------------------------------------------------------: | :--------: | :--------: | :--------: | :--------: |
|          [internlm/Intern-S1](https://huggingface.co/internlm/Intern-S1)          |     8      |     8      |     8      |     4      |
|      [internlm/Intern-S1-FP8](https://huggingface.co/internlm/Intern-S1-FP8)      |     -      |     4      |     4      |     2      |
|     [internlm/Intern-S1-mini](https://huggingface.co/internlm/Intern-S1-mini)     |     1      |     1      |     1      |     1      |
| [internlm/Intern-S1-mini-FP8](https://huggingface.co/internlm/Intern-S1-mini-FP8) |     -      |     1      |     1      |     1      |

ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹è¿™äº› LLM æ¨ç†å¼•æ“æ¥åˆ›å»ºä¸€ä¸ªå…¼å®¹ OpenAI åè®®çš„æœåŠ¡:

#### [lmdeploy(>=0.9.2)](https://github.com/InternLM/lmdeploy)

```bash
lmdeploy serve api_server internlm/Intern-S1 --reasoning-parser intern-s1 --tool-call-parser intern-s1 --tp 8
```

#### [vllm](https://github.com/vllm-project/vllm)

```bash
vllm serve internlm/Intern-S1 --tensor-parallel-size 8 --trust-remote-code
```

#### [sglang](https://github.com/sgl-project/sglang)

```bash
python3 -m sglang.launch_server \
    --model-path internlm/Intern-S1 \
    --trust-remote-code \
    --tp 8 \
    --grammar-backend none
```

#### ollama

```bash
# install ollama
curl -fsSL https://ollama.com/install.sh | sh
# fetch model
ollama pull internlm/interns1
# run model
ollama run internlm/interns1
# then use openai client to call on http://localhost:11434/v1
```

## é«˜çº§ç”¨æ³•

### å·¥å…·è°ƒç”¨ï¼ˆTool Callingï¼‰

è®¸å¤šå¤§å‹è¯­è¨€æ¨¡å‹ç°åœ¨å…·å¤‡äº† **å·¥å…·è°ƒç”¨ï¼ˆTool Callingï¼‰** çš„èƒ½åŠ›ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿé€šè¿‡ä¸å¤–éƒ¨å·¥å…·å’Œ API çš„äº¤äº’æ¥æ‰©å±•è‡ªèº«çš„èƒ½åŠ›ã€‚è¿™ä½¿å¾—æ¨¡å‹å¯ä»¥æ‰§è¡Œå¦‚è·å–æœ€æ–°ä¿¡æ¯ã€è¿è¡Œä»£ç ï¼Œæˆ–è°ƒç”¨å…¶ä»–åº”ç”¨ç¨‹åºä¸­çš„å‡½æ•°ç­‰ä»»åŠ¡ã€‚

å¯¹å¼€å‘è€…æ¥è¯´ï¼Œè¶Šæ¥è¶Šå¤šçš„å¼€æºè¯­è¨€æ¨¡å‹è®¾è®¡ä¸ºå…¼å®¹ OpenAI APIã€‚è¿™æ„å‘³ç€ä½ å¯ä»¥å¤ç”¨ OpenAI çš„æ¥å£ï¼Œåœ¨è¿™äº›å¼€æºæ¨¡å‹ä¸­å®ç°å·¥å…·è°ƒç”¨ã€‚å› æ­¤ï¼Œæœ¬æ•™ç¨‹ä¸­æ¼”ç¤ºçš„ä»£ç å…·æœ‰é«˜åº¦çš„é€šç”¨æ€§â€”â€”ä¸ä»…é€‚ç”¨äº OpenAI çš„æ¨¡å‹ï¼Œä¹Ÿé€‚ç”¨äºä»»ä½•éµå¾ªç›¸åŒæ¥å£æ ‡å‡†çš„æ¨¡å‹ã€‚

ä¸‹é¢æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå®é™…çš„ä»£ç ç¤ºä¾‹ï¼Œæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨å·¥å…·è°ƒç”¨åŠŸèƒ½æ¥è·å–æœ€æ–°çš„å¤©æ°”é¢„æŠ¥ï¼ˆåŸºäº lmdeploy api serverï¼‰ã€‚

```python

from openai import OpenAI
import json


def get_current_temperature(location: str, unit: str = "celsius"):
    """Get current temperature at a location.

    Args:
        location: The location to get the temperature for, in the format "City, State, Country".
        unit: The unit to return the temperature in. Defaults to "celsius". (choices: ["celsius", "fahrenheit"])

    Returns:
        the temperature, the location, and the unit in a dict
    """
    return {
        "temperature": 26.1,
        "location": location,
        "unit": unit,
    }


def get_temperature_date(location: str, date: str, unit: str = "celsius"):
    """Get temperature at a location and date.

    Args:
        location: The location to get the temperature for, in the format "City, State, Country".
        date: The date to get the temperature for, in the format "Year-Month-Day".
        unit: The unit to return the temperature in. Defaults to "celsius". (choices: ["celsius", "fahrenheit"])

    Returns:
        the temperature, the location, the date and the unit in a dict
    """
    return {
        "temperature": 25.9,
        "location": location,
        "date": date,
        "unit": unit,
    }

def get_function_by_name(name):
    if name == "get_current_temperature":
        return get_current_temperature
    if name == "get_temperature_date":
        return get_temperature_date

tools = [{
    'type': 'function',
    'function': {
        'name': 'get_current_temperature',
        'description': 'Get current temperature at a location.',
        'parameters': {
            'type': 'object',
            'properties': {
                'location': {
                    'type': 'string',
                    'description': 'The location to get the temperature for, in the format \'City, State, Country\'.'
                },
                'unit': {
                    'type': 'string',
                    'enum': [
                        'celsius',
                        'fahrenheit'
                    ],
                    'description': 'The unit to return the temperature in. Defaults to \'celsius\'.'
                }
            },
            'required': [
                'location'
            ]
        }
    }
}, {
    'type': 'function',
    'function': {
        'name': 'get_temperature_date',
        'description': 'Get temperature at a location and date.',
        'parameters': {
            'type': 'object',
            'properties': {
                'location': {
                    'type': 'string',
                    'description': 'The location to get the temperature for, in the format \'City, State, Country\'.'
                },
                'date': {
                    'type': 'string',
                    'description': 'The date to get the temperature for, in the format \'Year-Month-Day\'.'
                },
                'unit': {
                    'type': 'string',
                    'enum': [
                        'celsius',
                        'fahrenheit'
                    ],
                    'description': 'The unit to return the temperature in. Defaults to \'celsius\'.'
                }
            },
            'required': [
                'location',
                'date'
            ]
        }
    }
}]



messages = [
    {'role': 'user', 'content': 'Today is 2024-11-14, What\'s the temperature in San Francisco now? How about tomorrow?'}
]

openai_api_key = "EMPTY"
openai_api_base = "http://0.0.0.0:23333/v1"
client = OpenAI(
    api_key=openai_api_key,
    base_url=openai_api_base,
)
model_name = client.models.list().data[0].id
response = client.chat.completions.create(
    model=model_name,
    messages=messages,
    max_tokens=32768,
    temperature=0.8,
    top_p=0.8,
    stream=False,
    extra_body=dict(spaces_between_special_tokens=False, enable_thinking=False),
    tools=tools)
print(response.choices[0].message)
messages.append(response.choices[0].message)

for tool_call in response.choices[0].message.tool_calls:
    tool_call_args = json.loads(tool_call.function.arguments)
    tool_call_result = get_function_by_name(tool_call.function.name)(**tool_call_args)
    tool_call_result = json.dumps(tool_call_result, ensure_ascii=False)
    messages.append({
        'role': 'tool',
        'name': tool_call.function.name,
        'content': tool_call_result,
        'tool_call_id': tool_call.id
    })

response = client.chat.completions.create(
    model=model_name,
    messages=messages,
    temperature=0.8,
    top_p=0.8,
    stream=False,
    extra_body=dict(spaces_between_special_tokens=False, enable_thinking=False),
    tools=tools)
print(response.choices[0].message.content)
```

### åˆ‡æ¢æ·±åº¦æ€è€ƒæ¨¡å¼ä¸éæ€è€ƒæ¨¡å¼

Intern-S1 é»˜è®¤å¯ç”¨â€œæ·±åº¦æ€è€ƒæ¨¡å¼ï¼ˆthinking modeï¼‰â€ï¼Œè¯¥æ¨¡å¼å¯å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä»è€Œç”Ÿæˆæ›´é«˜è´¨é‡çš„å›å¤ã€‚è‹¥å¸Œæœ›å…³é—­æ­¤åŠŸèƒ½ï¼Œåªéœ€åœ¨ `tokenizer.apply_chat_template` ä¸­è®¾ç½®å‚æ•° `enable_thinking=False` å³å¯ã€‚

```python
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
    enable_thinking=False  # think mode indicator
)
```

ä½¿ç”¨ **LMDeploy** éƒ¨ç½² Intern-S1 æ¨¡å‹æ—¶ï¼Œä½ å¯ä»¥é€šè¿‡è°ƒæ•´è¯·æ±‚ä¸­çš„ `enable_thinking` å‚æ•°æ¥åŠ¨æ€æ§åˆ¶æ·±åº¦æ€è€ƒæ¨¡å¼çš„å¼€å¯ä¸å…³é—­ã€‚

```python
from openai import OpenAI
import json

messages = [
{
    'role': 'user',
    'content': 'who are you'
}, {
    'role': 'assistant',
    'content': 'I am an AI'
}, {
    'role': 'user',
    'content': 'AGI is?'
}]

openai_api_key = "EMPTY"
openai_api_base = "http://0.0.0.0:23333/v1"
client = OpenAI(
    api_key=openai_api_key,
    base_url=openai_api_base,
)
model_name = client.models.list().data[0].id

response = client.chat.completions.create(
    model=model_name,
    messages=messages,
    temperature=0.7,
    top_p=0.8,
    max_tokens=2048,
    extra_body={
        "enable_thinking": False,
    }
)
print(json.dumps(response.model_dump(), indent=2, ensure_ascii=False))
```

å¯¹äº **vllm** å’Œ **sglang** ç”¨æˆ·ï¼Œå¯é€šè¿‡ä»¥ä¸‹æ–¹å¼è¿›è¡Œé…ç½®ï¼š

```python
extra_body={
    "chat_template_kwargs": {"enable_thinking": False}
}
```

## å¼€æºåè®®

è¯¥é¡¹ç›®ä½¿ç”¨ Apache 2.0 å¼€æºåè®®ã€‚
